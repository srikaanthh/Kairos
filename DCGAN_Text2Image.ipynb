{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1aea55a",
   "metadata": {},
   "source": [
    "# DCGAN Text-to-Image Generation\n",
    "\n",
    "This notebook implements a Deep Convolutional Generative Adversarial Network (DCGAN) for text-to-image generation.\n",
    "\n",
    "## Features\n",
    "-  **Image size**: 64x64\n",
    "-  **Batch size**: 512\n",
    "-  **Epochs**: 70 (optimized for faster training)\n",
    "-  **Proper loss tracking**\n",
    "-  **Automatic checkpointing every 10 epochs**\n",
    "-  **Model saving and loading**\n",
    "-  **Training progress visualization**\n",
    "\n",
    "## Notebook Structure\n",
    "1. **[Setup & Imports](#1-setup--imports)** - Environment setup and library imports\n",
    "2. **[Data Loading](#2-data-loading)** - Dataset preparation and data loaders\n",
    "3. **[Model Architecture](#3-model-architecture)** - Generator and Discriminator models\n",
    "4. **[Training Configuration](#4-training-configuration)** - Hyperparameters and training setup\n",
    "5. **[Training Loop](#5-training-loop)** - Main training with checkpointing\n",
    "6. **[Evaluation & Visualization](#6-evaluation--visualization)** - Results analysis and image generation\n",
    "\n",
    "## Important Notes\n",
    "- **Single Training Loop**: This notebook contains only ONE training loop with proper checkpointing\n",
    "- **Automatic Saving**: Models are saved every 10 epochs and at the end\n",
    "- **No Duplicates**: All duplicate training loops have been removed\n",
    "- **Organized Structure**: Clear markdown sections for easy navigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9eeec",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "This section sets up the environment, imports necessary libraries, and configures the training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34c47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device cuda\n",
      "Training will run for 40 epochs\n",
      "Batch size: 512\n",
      "Image size: 64x64\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from utils import process_caption, weights_init\n",
    "from models.char_cnn_rnn_model import CharCNNRNN\n",
    "from models.dcgan_model import Generator, Discriminator\n",
    "\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using Device\", device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Date and paths\n",
    "date = datetime.now().strftime('%Y%m%d')\n",
    "start_time = time.time()\n",
    "\n",
    "output_save_path = './generated_images/'\n",
    "model_save_path = './saved_models/'\n",
    "os.makedirs(output_save_path, exist_ok=True)\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Parameters - \n",
    "noise_dim = 100\n",
    "embed_dim = 1024\n",
    "embed_out_dim = 128\n",
    "batch_size = 512  \n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "learning_rate = 0.0002\n",
    "l1_coef = 50\n",
    "l2_coef = 100\n",
    "\n",
    "num_epochs = 40  # Updated to 40 as 70 had convergence patterns (overkill)\n",
    "log_interval = 50\n",
    "\n",
    "print(f\"Training will run for {num_epochs} epochs\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Image size: 64x64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0780708",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "This section loads and prepares the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd31a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([512, 3, 64, 64]) torch.Size([512, 1024])\n",
      "Val batch: torch.Size([512, 3, 64, 64]) torch.Size([512, 1024])\n",
      "Test batch: torch.Size([512, 3, 64, 64]) torch.Size([512, 1024])\n",
      "Data loaders initialized successfully!\n",
      "Image size: 64x64\n",
      "Batch size: 512\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# ============================================\n",
    "# Load GloVe (300d)\n",
    "# ============================================\n",
    "def load_glove(path=\"glove.6B.300d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Make sure glove.6B.300d.txt is in the same folder as your notebook, or set full path\n",
    "glove = load_glove(\"glove.6B.300d.txt\")\n",
    "\n",
    "# Project GloVe 300-dim -> 1024-dim (matches Generator/Discriminator embed_dim)\n",
    "proj_to_1024 = nn.Linear(300, 1024)\n",
    "\n",
    "def caption_to_embedding(caption):\n",
    "    words = caption.lower().split()\n",
    "    vecs = [glove[w] for w in words if w in glove]\n",
    "    if not vecs:\n",
    "        vecs = [np.zeros(300)]\n",
    "    avg_vec = np.mean(vecs, axis=0)\n",
    "    avg_vec = torch.tensor(avg_vec, dtype=torch.float32).unsqueeze(0)\n",
    "    return proj_to_1024(avg_vec).squeeze(0)\n",
    "\n",
    "# ============================================\n",
    "# COCO Dataset Class\n",
    "# ============================================\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, img_root, captions_json, transform=None, filter_word=None):\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "        with open(captions_json, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.images = {img['id']: img['file_name'] for img in data['images']}\n",
    "        # optional filtering (e.g., only keep captions with \"doctor\")\n",
    "        if filter_word:\n",
    "            self.captions = [c for c in data['annotations'] if filter_word in c['caption'].lower()]\n",
    "        else:\n",
    "            self.captions = data['annotations']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]['caption']\n",
    "        img_id = self.captions[idx]['image_id']\n",
    "        img_path = os.path.join(self.img_root, self.images[img_id])\n",
    "        # image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = PILImage.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        embed = caption_to_embedding(caption)\n",
    "        return {\"right_images\": image, \"right_embed\": embed}\n",
    "\n",
    "# ============================================\n",
    "# Instantiate Train / Val / Test DataLoaders\n",
    "# ============================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Use consistent batch_size = 512\n",
    "batch_size = 512\n",
    "\n",
    "# ---- Train loader ----\n",
    "train_dataset = COCODataset(\n",
    "    img_root=r\"E:\\USF\\Trustworthy AI\\train2014\\train2014\",   # path to COCO training images\n",
    "    captions_json=r\"E:\\USF\\Trustworthy AI\\annotations_trainval2014\\annotations\\captions_train2014.json\", # Replace with directory\n",
    "    transform=transform,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# ---- Validation loader ----\n",
    "val_dataset = COCODataset(\n",
    "    img_root=r\"E:\\USF\\Trustworthy AI\\val2014\\val2014\",   # path to COCO validation images\n",
    "    captions_json=r\"E:\\USF\\Trustworthy AI\\annotations_trainval2014\\annotations\\captions_val2014.json\",\n",
    "    transform=transform,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---- Test loader ----\n",
    "test_dataset = COCODataset(\n",
    "    img_root=r\"E:\\USF\\Trustworthy AI\\val2014\\val2014\",\n",
    "    captions_json=r\"E:\\USF\\Trustworthy AI\\annotations_trainval2014\\annotations\\captions_val2014.json\",\n",
    "    transform=transform,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test data loaders\n",
    "train_batch = next(iter(train_loader))\n",
    "print(\"Train batch:\", train_batch['right_images'].shape, train_batch['right_embed'].shape)\n",
    "\n",
    "val_batch = next(iter(val_loader))\n",
    "print(\"Val batch:\", val_batch['right_images'].shape, val_batch['right_embed'].shape)\n",
    "\n",
    "test_batch = next(iter(test_loader))\n",
    "print(\"Test batch:\", test_batch['right_images'].shape, test_batch['right_embed'].shape)\n",
    "\n",
    "print(\"Data loaders initialized successfully!\")\n",
    "print(f\"Image size: {train_batch['right_images'].shape[2]}x{train_batch['right_images'].shape[3]}\")\n",
    "print(f\"Batch size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cea41",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e948b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models initialized with mixed precision support!\n",
      "Generator parameters: 4,756,736\n",
      "Discriminator parameters: 2,900,032\n",
      "✅ Mixed precision scaler initialized!\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE MODELS AND LOSS TRACKING\n",
    "# Loss functions\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "l2_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# Initialize loss tracking lists\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(channels=3, embed_dim=embed_dim, noise_dim=noise_dim, embed_out_dim=embed_out_dim).to(device)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "discriminator = Discriminator(channels=3, embed_dim=embed_dim, embed_out_dim=embed_out_dim).to(device)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# Setup optimizers with different learning rates\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Add schedulers\n",
    "scheduler_G = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_G, T_0=20, T_mult=2, eta_min=1e-4)\n",
    "scheduler_D = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_D, T_0=20, T_mult=2, eta_min=1e-4)\n",
    "\n",
    "# MIXED PRECISION SETUP\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"Models initialized with mixed precision support!\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(\"Mixed precision scaler initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f8224",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc8a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Load a checkpoint and continue training with new hyperparameters\n",
    "# checkpoint_path = 'saved_models/checkpoint_epoch_50.pth'\n",
    "# start_epoch, G_losses, D_losses = load_checkpoint(\n",
    "#     checkpoint_path, generator, discriminator, optimizer_G, optimizer_D\n",
    "# )\n",
    "\n",
    "# Update learning rate for fine-tuning\n",
    "for param_group in optimizer_G.param_groups:\n",
    "    param_group['lr'] = 0.0001  # Lower learning rate\n",
    "\n",
    "for param_group in optimizer_D.param_groups:\n",
    "    param_group['lr'] = 0.0001\n",
    "\n",
    "# Continue training\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D, G_losses, D_losses, model_save_path):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'G_losses': G_losses,\n",
    "        'D_losses': D_losses,\n",
    "        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_save_path, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_path, generator, discriminator, optimizer_G, optimizer_D):\n",
    "    \"\"\"Load training checkpoint\"\"\"\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Restore models\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    # Restore optimizers\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    \n",
    "    # Restore training state\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    G_losses = checkpoint['G_losses']\n",
    "    D_losses = checkpoint['D_losses']\n",
    "    \n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "    print(f\"Previous losses - G: {G_losses[-1]:.4f}, D: {D_losses[-1]:.4f}\")\n",
    "    \n",
    "    return start_epoch, G_losses, D_losses\n",
    "\n",
    "# Function to save final models\n",
    "def save_final_models(generator, discriminator, model_save_path):\n",
    "    \"\"\"Save final trained models\"\"\"\n",
    "    generator_path = os.path.join(model_save_path, 'generator_final.pth')\n",
    "    discriminator_path = os.path.join(model_save_path, 'discriminator_final.pth')\n",
    "    \n",
    "    torch.save(generator.state_dict(), generator_path)\n",
    "    torch.save(discriminator.state_dict(), discriminator_path)\n",
    "    \n",
    "    print(f\"Final models saved:\")\n",
    "    print(f\"Generator: {generator_path}\")\n",
    "    print(f\"Discriminator: {discriminator_path}\")\n",
    "    \n",
    "    return generator_path, discriminator_path\n",
    "\n",
    "def feature_matching_loss(fake_features, real_features):\n",
    "    \"\"\"Feature matching loss for better generator training\"\"\"\n",
    "    loss = 0\n",
    "    for fake_feat, real_feat in zip(fake_features, real_features):\n",
    "        # Ensure both tensors are the same dtype\n",
    "        if fake_feat.dtype != real_feat.dtype:\n",
    "            real_feat = real_feat.to(fake_feat.dtype)\n",
    "        loss += F.mse_loss(fake_feat, real_feat)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddecaa7",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405226b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 40 epochs...\n",
      "Mixed precision: ENABLED\n",
      "Checkpoint saving every 10 epochs\n",
      "Training will save to: ./saved_models/\n",
      "Epoch [1/40] Batch 50/809  D: 0.0006  G: 16.2164\n",
      "Epoch [1/40] Batch 100/809  D: 0.1681  G: 15.1700\n",
      "Epoch [1/40] Batch 150/809  D: 0.5826  G: 9.5338\n",
      "Epoch [1/40] Batch 200/809  D: 0.1350  G: 11.9285\n",
      "Epoch [1/40] Batch 250/809  D: 0.6982  G: 9.4538\n",
      "Epoch [1/40] Batch 300/809  D: 0.5778  G: 9.1658\n",
      "Epoch [1/40] Batch 350/809  D: 0.2750  G: 8.0645\n",
      "Epoch [1/40] Batch 400/809  D: 0.1146  G: 11.2135\n",
      "Epoch [1/40] Batch 450/809  D: 0.2077  G: 12.8424\n",
      "Epoch [1/40] Batch 500/809  D: 0.1695  G: 8.1540\n",
      "Epoch [1/40] Batch 550/809  D: 0.6847  G: 9.7157\n",
      "Epoch [1/40] Batch 600/809  D: 0.1140  G: 6.5246\n",
      "Epoch [1/40] Batch 650/809  D: 0.6133  G: 9.1059\n",
      "Epoch [1/40] Batch 700/809  D: 0.3243  G: 9.4835\n",
      "Epoch [1/40] Batch 750/809  D: 1.2175  G: 6.7936\n",
      "Epoch [1/40] Batch 800/809  D: 0.5817  G: 10.8345\n",
      "Epoch 1 completed in 2008.80 seconds\n",
      "Current LR - G: 0.000994, D: 0.000199\n",
      "Epoch [2/40] Batch 50/809  D: 0.9668  G: 8.7395\n",
      "Epoch [2/40] Batch 100/809  D: 3.1814  G: 5.0512\n",
      "Epoch [2/40] Batch 150/809  D: 2.6736  G: 7.9216\n",
      "Epoch [2/40] Batch 200/809  D: 2.1275  G: 7.0206\n",
      "Epoch [2/40] Batch 250/809  D: 1.7451  G: 8.1575\n",
      "Epoch [2/40] Batch 300/809  D: 1.8747  G: 5.4184\n",
      "Epoch [2/40] Batch 350/809  D: 1.9226  G: 6.1674\n",
      "Epoch [2/40] Batch 400/809  D: 0.7929  G: 7.2530\n",
      "Epoch [2/40] Batch 450/809  D: 1.5141  G: 7.4965\n",
      "Epoch [2/40] Batch 500/809  D: 1.3407  G: 5.3751\n",
      "Epoch [2/40] Batch 550/809  D: 1.1746  G: 4.8843\n",
      "Epoch [2/40] Batch 600/809  D: 1.5953  G: 4.4996\n",
      "Epoch [2/40] Batch 650/809  D: 1.0779  G: 5.1304\n",
      "Epoch [2/40] Batch 700/809  D: 1.3399  G: 4.1243\n",
      "Epoch [2/40] Batch 750/809  D: 1.1298  G: 4.5754\n",
      "Epoch [2/40] Batch 800/809  D: 1.4708  G: 4.5882\n",
      "Epoch 2 completed in 1468.35 seconds\n",
      "Current LR - G: 0.000978, D: 0.000198\n",
      "Epoch [3/40] Batch 50/809  D: 1.1510  G: 4.6969\n",
      "Epoch [3/40] Batch 100/809  D: 1.2884  G: 4.4692\n",
      "Epoch [3/40] Batch 150/809  D: 1.8532  G: 4.6831\n",
      "Epoch [3/40] Batch 200/809  D: 1.3795  G: 4.7502\n",
      "Epoch [3/40] Batch 250/809  D: 1.5084  G: 3.9938\n",
      "Epoch [3/40] Batch 300/809  D: 1.5695  G: 4.0318\n",
      "Epoch [3/40] Batch 350/809  D: 1.0142  G: 3.9958\n",
      "Epoch [3/40] Batch 400/809  D: 1.0080  G: 3.8405\n",
      "Epoch [3/40] Batch 450/809  D: 1.0470  G: 3.7941\n",
      "Epoch [3/40] Batch 500/809  D: 0.8540  G: 4.1688\n",
      "Epoch [3/40] Batch 550/809  D: 1.1016  G: 4.3356\n",
      "Epoch [3/40] Batch 600/809  D: 1.8568  G: 3.4963\n",
      "Epoch [3/40] Batch 650/809  D: 1.1405  G: 3.6462\n",
      "Epoch [3/40] Batch 700/809  D: 1.4952  G: 3.8640\n",
      "Epoch [3/40] Batch 750/809  D: 1.3225  G: 3.7564\n",
      "Epoch [3/40] Batch 800/809  D: 1.1246  G: 4.2948\n",
      "Epoch 3 completed in 1465.74 seconds\n",
      "Current LR - G: 0.000951, D: 0.000195\n",
      "Epoch [4/40] Batch 50/809  D: 1.3127  G: 3.6626\n",
      "Epoch [4/40] Batch 100/809  D: 1.2403  G: 3.6477\n",
      "Epoch [4/40] Batch 150/809  D: 1.2706  G: 3.4557\n",
      "Epoch [4/40] Batch 200/809  D: 1.6290  G: 3.4055\n",
      "Epoch [4/40] Batch 250/809  D: 1.3596  G: 3.5699\n",
      "Epoch [4/40] Batch 300/809  D: 1.0867  G: 3.5162\n",
      "Epoch [4/40] Batch 350/809  D: 1.2596  G: 3.4585\n",
      "Epoch [4/40] Batch 400/809  D: 1.2486  G: 3.5605\n",
      "Epoch [4/40] Batch 450/809  D: 1.2949  G: 4.0133\n",
      "Epoch [4/40] Batch 500/809  D: 1.4077  G: 3.2836\n",
      "Epoch [4/40] Batch 550/809  D: 1.2482  G: 3.1691\n",
      "Epoch [4/40] Batch 600/809  D: 1.2759  G: 3.4144\n",
      "Epoch [4/40] Batch 650/809  D: 1.4029  G: 3.5570\n",
      "Epoch [4/40] Batch 700/809  D: 1.5255  G: 3.3651\n",
      "Epoch [4/40] Batch 750/809  D: 1.5304  G: 3.3748\n",
      "Epoch [4/40] Batch 800/809  D: 1.3203  G: 3.9122\n",
      "Epoch 4 completed in 1478.99 seconds\n",
      "Current LR - G: 0.000914, D: 0.000190\n",
      "Epoch [5/40] Batch 50/809  D: 1.1660  G: 4.1100\n",
      "Epoch [5/40] Batch 100/809  D: 1.4133  G: 3.2467\n",
      "Epoch [5/40] Batch 150/809  D: 1.3789  G: 2.9526\n",
      "Epoch [5/40] Batch 200/809  D: 1.4030  G: 3.6720\n",
      "Epoch [5/40] Batch 250/809  D: 1.3382  G: 3.5179\n",
      "Epoch [5/40] Batch 300/809  D: 1.3206  G: 3.5349\n",
      "Epoch [5/40] Batch 350/809  D: 1.3773  G: 3.3317\n",
      "Epoch [5/40] Batch 400/809  D: 1.3540  G: 3.0021\n",
      "Epoch [5/40] Batch 450/809  D: 1.3852  G: 3.5917\n",
      "Epoch [5/40] Batch 500/809  D: 1.3917  G: 2.9716\n",
      "Epoch [5/40] Batch 550/809  D: 1.3651  G: 2.9980\n",
      "Epoch [5/40] Batch 600/809  D: 1.3389  G: 3.3499\n",
      "Epoch [5/40] Batch 650/809  D: 1.3517  G: 3.3749\n",
      "Epoch [5/40] Batch 700/809  D: 1.4906  G: 3.2550\n",
      "Epoch [5/40] Batch 750/809  D: 1.5036  G: 3.0338\n",
      "Epoch [5/40] Batch 800/809  D: 1.4217  G: 3.1709\n",
      "Epoch 5 completed in 1471.97 seconds\n",
      "Current LR - G: 0.000868, D: 0.000185\n",
      "Epoch [6/40] Batch 50/809  D: 1.3366  G: 3.0220\n",
      "Epoch [6/40] Batch 100/809  D: 1.1976  G: 3.3882\n",
      "Epoch [6/40] Batch 150/809  D: 1.1899  G: 3.2238\n",
      "Epoch [6/40] Batch 200/809  D: 1.1926  G: 3.2605\n",
      "Epoch [6/40] Batch 250/809  D: 1.2405  G: 3.1150\n",
      "Epoch [6/40] Batch 300/809  D: 1.1542  G: 3.3580\n",
      "Epoch [6/40] Batch 350/809  D: 1.1407  G: 3.1513\n",
      "Epoch [6/40] Batch 400/809  D: 1.2131  G: 3.2622\n",
      "Epoch [6/40] Batch 450/809  D: 1.1468  G: 3.2246\n",
      "Epoch [6/40] Batch 500/809  D: 1.3605  G: 3.1740\n",
      "Epoch [6/40] Batch 550/809  D: 1.2245  G: 3.0711\n",
      "Epoch [6/40] Batch 600/809  D: 1.2762  G: 2.9530\n",
      "Epoch [6/40] Batch 650/809  D: 1.1988  G: 3.0887\n",
      "Epoch [6/40] Batch 700/809  D: 1.3147  G: 3.1141\n",
      "Epoch [6/40] Batch 750/809  D: 1.1801  G: 3.0827\n",
      "Epoch [6/40] Batch 800/809  D: 1.2321  G: 3.1412\n",
      "Epoch 6 completed in 1473.38 seconds\n",
      "Current LR - G: 0.000815, D: 0.000179\n",
      "Epoch [7/40] Batch 50/809  D: 1.2843  G: 3.0375\n",
      "Epoch [7/40] Batch 100/809  D: 1.3056  G: 2.8656\n",
      "Epoch [7/40] Batch 150/809  D: 1.2105  G: 2.9849\n",
      "Epoch [7/40] Batch 200/809  D: 1.1694  G: 3.2095\n",
      "Epoch [7/40] Batch 250/809  D: 1.2512  G: 3.3412\n",
      "Epoch [7/40] Batch 300/809  D: 1.1928  G: 3.1020\n",
      "Epoch [7/40] Batch 350/809  D: 1.2056  G: 3.4242\n",
      "Epoch [7/40] Batch 400/809  D: 1.3883  G: 3.4372\n",
      "Epoch [7/40] Batch 450/809  D: 1.6335  G: 2.7269\n",
      "Epoch [7/40] Batch 500/809  D: 1.3143  G: 3.5132\n",
      "Epoch [7/40] Batch 550/809  D: 1.2374  G: 3.4580\n",
      "Epoch [7/40] Batch 600/809  D: 1.3960  G: 3.0937\n",
      "Epoch [7/40] Batch 650/809  D: 1.3671  G: 2.9419\n",
      "Epoch [7/40] Batch 700/809  D: 1.1552  G: 3.4393\n",
      "Epoch [7/40] Batch 750/809  D: 1.3032  G: 3.0239\n",
      "Epoch [7/40] Batch 800/809  D: 1.2599  G: 3.4032\n",
      "Epoch 7 completed in 1470.44 seconds\n",
      "Current LR - G: 0.000754, D: 0.000173\n",
      "Epoch [8/40] Batch 50/809  D: 1.2382  G: 3.1167\n",
      "Epoch [8/40] Batch 100/809  D: 1.2534  G: 3.3791\n",
      "Epoch [8/40] Batch 150/809  D: 1.2715  G: 3.2191\n",
      "Epoch [8/40] Batch 200/809  D: 1.3221  G: 3.0746\n",
      "Epoch [8/40] Batch 250/809  D: 1.2579  G: 2.9461\n",
      "Epoch [8/40] Batch 300/809  D: 1.4017  G: 2.9938\n",
      "Epoch [8/40] Batch 350/809  D: 1.2892  G: 2.7180\n",
      "Epoch [8/40] Batch 400/809  D: 1.4873  G: 3.3630\n",
      "Epoch [8/40] Batch 450/809  D: 1.1987  G: 3.0860\n",
      "Epoch [8/40] Batch 500/809  D: 1.1133  G: 3.4306\n",
      "Epoch [8/40] Batch 550/809  D: 1.2201  G: 3.1574\n",
      "Epoch [8/40] Batch 600/809  D: 1.1587  G: 2.9620\n",
      "Epoch [8/40] Batch 650/809  D: 1.1509  G: 3.4495\n",
      "Epoch [8/40] Batch 700/809  D: 1.2661  G: 3.6443\n",
      "Epoch [8/40] Batch 750/809  D: 1.5764  G: 2.8488\n",
      "Epoch [8/40] Batch 800/809  D: 1.3464  G: 3.4068\n",
      "Epoch 8 completed in 1671.80 seconds\n",
      "Current LR - G: 0.000689, D: 0.000165\n",
      "Epoch [9/40] Batch 50/809  D: 1.3225  G: 2.9227\n",
      "Epoch [9/40] Batch 100/809  D: 1.4878  G: 2.7864\n",
      "Epoch [9/40] Batch 150/809  D: 1.3257  G: 3.4558\n",
      "Epoch [9/40] Batch 200/809  D: 1.1579  G: 3.1850\n",
      "Epoch [9/40] Batch 250/809  D: 1.2205  G: 3.3853\n",
      "Epoch [9/40] Batch 300/809  D: 1.3193  G: 3.5581\n",
      "Epoch [9/40] Batch 350/809  D: 1.5177  G: 3.2012\n",
      "Epoch [9/40] Batch 400/809  D: 1.2847  G: 3.5111\n",
      "Epoch [9/40] Batch 450/809  D: 1.3220  G: 3.0439\n",
      "Epoch [9/40] Batch 500/809  D: 1.2989  G: 2.9452\n",
      "Epoch [9/40] Batch 550/809  D: 1.2720  G: 3.0724\n",
      "Epoch [9/40] Batch 600/809  D: 1.3004  G: 2.7742\n",
      "Epoch [9/40] Batch 650/809  D: 1.3582  G: 2.8040\n",
      "Epoch [9/40] Batch 700/809  D: 1.4584  G: 3.0181\n",
      "Epoch [9/40] Batch 750/809  D: 1.3521  G: 2.9419\n",
      "Epoch [9/40] Batch 800/809  D: 1.3205  G: 3.9450\n",
      "Epoch 9 completed in 1514.88 seconds\n",
      "Current LR - G: 0.000620, D: 0.000158\n",
      "Epoch [10/40] Batch 50/809  D: 1.3097  G: 3.2080\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Training Loop with Mixed Precision\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Mixed precision: ENABLED\")\n",
    "print(f\"Checkpoint saving every 10 epochs\")\n",
    "print(f\"Training will save to: {model_save_path}\")\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            images = batch['right_images'].to(device, non_blocking=True)\n",
    "            embeddings = batch['right_embed'].to(device, non_blocking=True)\n",
    "            bsz = images.size(0)\n",
    "\n",
    "            # -----------------------------\n",
    "            # 1) Train Discriminator\n",
    "            # -----------------------------\n",
    "            optimizer_D.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Generate fakes (keep graph for G step)\n",
    "            noise = torch.randn(bsz, noise_dim, 1, 1, device=device)\n",
    "            \n",
    "            # MIXED PRECISION: Wrap forward pass in autocast\n",
    "            with autocast():\n",
    "                fake_images = generator(noise, embeddings)\n",
    "                emb_D = embeddings.detach()\n",
    "\n",
    "                # Real pair\n",
    "                real_out, _ = discriminator(images, emb_D)\n",
    "                real_targets = torch.ones_like(real_out)\n",
    "                d_loss_real = criterion(real_out, real_targets)\n",
    "\n",
    "                # Fake pair\n",
    "                fake_out, _ = discriminator(fake_images.detach(), emb_D)\n",
    "                fake_targets = torch.zeros_like(fake_out)\n",
    "                d_loss_fake = criterion(fake_out, fake_targets)\n",
    "\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            # MIXED PRECISION: Use scaler for backward pass\n",
    "            scaler.scale(d_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            scaler.step(optimizer_D)\n",
    "\n",
    "            # -----------------------------\n",
    "            # 2) Train Generator\n",
    "            # -----------------------------\n",
    "            optimizer_G.zero_grad(set_to_none=True)\n",
    "\n",
    "            # MIXED PRECISION: Wrap forward pass in autocast\n",
    "            with autocast():\n",
    "                out_fake, fake_features = discriminator(fake_images, embeddings)\n",
    "                g_targets = torch.ones_like(out_fake)\n",
    "                g_loss = criterion(out_fake, g_targets)\n",
    "\n",
    "            # Feature matching loss\n",
    "            with torch.no_grad():\n",
    "                _, real_features = discriminator(images, embeddings.detach())\n",
    "            feature_loss = feature_matching_loss(fake_features, real_features)\n",
    "            g_loss = g_loss + 0.01 * feature_loss\n",
    "            \n",
    "\n",
    "            # MIXED PRECISION: Use scaler for backward pass\n",
    "            scaler.scale(g_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            scaler.step(optimizer_G)\n",
    "            \n",
    "            #  MIXED PRECISION: Update scaler after both steps\n",
    "            scaler.update()\n",
    "\n",
    "            # Store losses\n",
    "            D_losses.append(d_loss.item())\n",
    "            G_losses.append(g_loss.item())\n",
    "\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                    f\"Batch {batch_idx+1}/{len(train_loader)}  \"\n",
    "                    f\"D: {d_loss.item():.4f}  G: {g_loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = save_checkpoint(\n",
    "                epoch, generator, discriminator, optimizer_G, optimizer_D, \n",
    "                G_losses, D_losses, model_save_path\n",
    "            )\n",
    "            \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # Step learning rate schedulers\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        \n",
    "        # Print current learning rates\n",
    "        current_lr_G = optimizer_G.param_groups[0]['lr']\n",
    "        current_lr_D = optimizer_D.param_groups[0]['lr']\n",
    "        print(f\"Current LR - G: {current_lr_G:.6f}, D: {current_lr_D:.6f}\")\n",
    "\n",
    "    # Save final models\n",
    "    print(\"Training completed! Saving final models...\")\n",
    "    generator_path, discriminator_path = save_final_models(generator, discriminator, model_save_path)\n",
    "    \n",
    "    print(f\"Total training time: {(time.time() - start_time)/3600:.2f} hours\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted! Saving current state...\")\n",
    "    # Save interrupted state\n",
    "    interrupted_path = os.path.join(model_save_path, 'interrupted_training.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'scheduler_G_state_dict': scheduler_G.state_dict(),\n",
    "        'scheduler_D_state_dict': scheduler_D.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),  # Save scaler state\n",
    "        'G_losses': G_losses,\n",
    "        'D_losses': D_losses,\n",
    "        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }, interrupted_path)\n",
    "    print(f\"Interrupted state saved to: {interrupted_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining error: {e}\")\n",
    "    print(\"Saving error state...\")\n",
    "    error_path = os.path.join(model_save_path, 'error_training.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'scheduler_G_state_dict': scheduler_G.state_dict(),\n",
    "        'scheduler_D_state_dict': scheduler_D.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),  # Save scaler state\n",
    "        'G_losses': G_losses,\n",
    "        'D_losses': D_losses,\n",
    "        'error': str(e),\n",
    "        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }, error_path)\n",
    "    print(f\"Error state saved to: {error_path}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced Training Loop with Checkpointing\n",
    "# print(f\"Starting training for {num_epochs} epochs...\")\n",
    "# print(f\"Checkpoint saving every 10 epochs\")\n",
    "# print(f\"Training will save to: {model_save_path}\")\n",
    "\n",
    "# generator.train()\n",
    "# discriminator.train()\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_start_time = time.time()\n",
    "        \n",
    "#         for batch_idx, batch in enumerate(train_loader):\n",
    "#             images = batch['right_images'].to(device, non_blocking=True)\n",
    "#             embeddings = batch['right_embed'].to(device, non_blocking=True)\n",
    "#             bsz = images.size(0)\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # 1) Train Discriminator\n",
    "#             # -----------------------------\n",
    "#             optimizer_D.zero_grad(set_to_none=True)\n",
    "\n",
    "#             # Generate fakes (keep graph for G step)\n",
    "#             noise = torch.randn(bsz, noise_dim, 1, 1, device=device)\n",
    "            \n",
    "#             with autocast(device_type='cuda'):\n",
    "#                 fake_images = generator(noise, embeddings)\n",
    "#                 emb_D = embeddings.detach()\n",
    "\n",
    "#                 # Real pair\n",
    "#                 real_out, _ = discriminator(images, emb_D)\n",
    "#                 real_targets = torch.ones_like(real_out)\n",
    "#                 d_loss_real = criterion(real_out, real_targets)\n",
    "\n",
    "#                 # Fake pair\n",
    "#                 fake_out, _ = discriminator(fake_images.detach(), emb_D)\n",
    "#                 fake_targets = torch.zeros_like(fake_out)\n",
    "#                 d_loss_fake = criterion(fake_out, fake_targets)\n",
    "\n",
    "#                 d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "#             scaler.scale(d_loss).backward()\n",
    "#             scaler.step(optimizer_D)\n",
    "\n",
    "#             # -----------------------------\n",
    "#             # 2) Train Generator\n",
    "#             # -----------------------------\n",
    "#             optimizer_G.zero_grad(set_to_none=True)\n",
    "\n",
    "#             with autocast(device_type='cuda'):\n",
    "#                 out_fake, _ = discriminator(fake_images, embeddings)\n",
    "#                 g_targets = torch.ones_like(out_fake)\n",
    "#                 g_loss = criterion(out_fake, g_targets)\n",
    "\n",
    "#             scaler.scale(g_loss).backward()\n",
    "#             scaler.step(optimizer_G)\n",
    "#             scaler.update()\n",
    "\n",
    "#             # Store losses\n",
    "#             D_losses.append(d_loss.item())\n",
    "#             G_losses.append(g_loss.item())\n",
    "\n",
    "#             if (batch_idx + 1) % log_interval == 0:\n",
    "#                 print(\n",
    "#                     f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "#                     f\"Batch {batch_idx+1}/{len(train_loader)}  \"\n",
    "#                     f\"D: {d_loss.item():.4f}  G: {g_loss.item():.4f}\"\n",
    "#                 )\n",
    "\n",
    "#         # Save checkpoint every 10 epochs\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             checkpoint_path = save_checkpoint(\n",
    "#                 epoch, generator, discriminator, optimizer_G, optimizer_D, \n",
    "#                 G_losses, D_losses, model_save_path\n",
    "#             )\n",
    "            \n",
    "#         epoch_time = time.time() - epoch_start_time\n",
    "#         print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
    "\n",
    "#     # Save final models\n",
    "#     print(\"Training completed! Saving final models...\")\n",
    "#     generator_path, discriminator_path = save_final_models(generator, discriminator, model_save_path)\n",
    "    \n",
    "#     print(f\"Total training time: {(time.time() - start_time)/3600:.2f} hours\")\n",
    "    \n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\\\nTraining interrupted! Saving current state...\")\n",
    "#     # Save interrupted state\n",
    "#     interrupted_path = os.path.join(model_save_path, 'interrupted_training.pth')\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'generator_state_dict': generator.state_dict(),\n",
    "#         'discriminator_state_dict': discriminator.state_dict(),\n",
    "#         'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "#         'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "#         'G_losses': G_losses,\n",
    "#         'D_losses': D_losses,\n",
    "#         'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     }, interrupted_path)\n",
    "#     print(f\"Interrupted state saved to: {interrupted_path}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"\\\\nTraining error: {e}\")\n",
    "#     print(\"Saving error state...\")\n",
    "#     error_path = os.path.join(model_save_path, 'error_training.pth')\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'generator_state_dict': generator.state_dict(),\n",
    "#         'discriminator_state_dict': discriminator.state_dict(),\n",
    "#         'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "#         'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "#         'G_losses': G_losses,\n",
    "#         'D_losses': D_losses,\n",
    "#         'error': str(e),\n",
    "#         'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     }, error_path)\n",
    "#     print(f\"Error state saved to: {error_path}\")\n",
    "#     raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c06324",
   "metadata": {},
   "source": [
    "## Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec88bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained generator model and generate sample images\n",
    "print(\"Loading trained generator model...\")\n",
    "generator_path = os.path.join(model_save_path, 'generator_final.pth')\n",
    "if os.path.exists(generator_path):\n",
    "    generator.load_state_dict(torch.load(generator_path, map_location=device))\n",
    "    generator.eval()\n",
    "    print(\"Generator model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Generator model not found. Please run training first.\")\n",
    "    raise FileNotFoundError(\"Generator model not found\")\n",
    "\n",
    "# Generate sample images for visualization\n",
    "print(\"Generating sample images...\")\n",
    "num_samples = 20  # Number of images to generate for the GIF\n",
    "sample_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        # Generate random noise with correct dimensions (batch_size, channels, height, width)\n",
    "        noise = torch.randn(1, noise_dim, 1, 1, device=device)\n",
    "        \n",
    "        # Generate random text embedding (using zeros for now since we don't have text input)\n",
    "        text_embedding = torch.zeros(1, embed_dim, device=device)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_image = generator(noise, text_embedding)\n",
    "        \n",
    "        # Convert to numpy and save\n",
    "        img_np = generated_image.squeeze().cpu().numpy()\n",
    "        img_np = (img_np + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        # Fix image dimensions for PIL - ensure it's (height, width, channels)\n",
    "        if len(img_np.shape) == 3 and img_np.shape[0] == 3:  # (3, H, W) -> (H, W, 3)\n",
    "            img_np = np.transpose(img_np, (1, 2, 0))\n",
    "        elif len(img_np.shape) == 2:  # (H, W) -> (H, W, 1) then (H, W, 3)\n",
    "            img_np = np.stack([img_np, img_np, img_np], axis=2)\n",
    "        \n",
    "        # Convert to PIL Image and save\n",
    "        img_pil = img_pil = PILImage.fromarray((img_np * 255).astype(np.uint8))\n",
    "        img_filename = f'output_{date}_{i:03d}.png'\n",
    "        img_pil.save(os.path.join(output_save_path, img_filename))\n",
    "        sample_images.append(img_np)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Generated {i + 1}/{num_samples} images\")\n",
    "\n",
    "print(f\"Generated {num_samples} sample images!\")\n",
    "\n",
    "# Create GIF from the generated images\n",
    "if sample_images:\n",
    "    print(\"Creating GIF from generated images...\")\n",
    "    # Convert numpy arrays to the format expected by imageio\n",
    "    gif_images = []\n",
    "    for img in sample_images:\n",
    "        # Convert to uint8 format\n",
    "        img_uint8 = (img * 255).astype(np.uint8)\n",
    "        gif_images.append(img_uint8)\n",
    "    \n",
    "    # Save as GIF\n",
    "    gif_path = os.path.join(output_save_path, f'output_gif_{date}.gif')\n",
    "    imageio.mimsave(gif_path, gif_images, fps=1)\n",
    "    print(f\"GIF saved to: {gif_path}\")\n",
    "else:\n",
    "    print(\"No images to create GIF from!\")\n",
    "\n",
    "# Display some sample generated images\n",
    "print(\"\\nDisplaying sample generated images:\")\n",
    "import matplotlib.pyplot as plt\n",
    "# from IPython.display import Image, display\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Generated Images from Trained DCGAN', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    \n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i]\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(f'Image {i+1}')\n",
    "        axes[row, col].axis('off')\n",
    "    else:\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the GIF\n",
    "if os.path.exists(gif_path):\n",
    "    print(f\"\\nDisplaying GIF: {gif_path}\")\n",
    "    # display(Image(gif_path))\n",
    "    display(IPyImage(gif_path))\n",
    "else:\n",
    "    print(\"GIF not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ce0d7",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea10cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "# Set environment variables to prevent OpenMP conflicts\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Import in correct order\n",
    "import torch\n",
    "from torchvision.models import inception_v3\n",
    "from torch.nn import functional as F\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "# FID (Fréchet Inception Distance) Calculation\n",
    "class FIDCalculator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        # Load pre-trained Inception v3 with robust loading\n",
    "        try:\n",
    "            from torchvision.models import Inception_V3_Weights\n",
    "            self.inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=False).to(device)\n",
    "        except Exception:\n",
    "            self.inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "        self.inception.eval()\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.inception.fc = nn.Identity()\n",
    "        \n",
    "        # Add resize layer for 32x32 images to work with Inception v3 (expects 299x299)\n",
    "        self.resize = transforms.Resize((299, 299))\n",
    "        \n",
    "    def get_features(self, images):\n",
    "        \"\"\"Extract features from images using Inception v3\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Resize 32x32 images to 299x299 for Inception v3\n",
    "            resized_images = self.resize(images)\n",
    "            features = self.inception(resized_images)\n",
    "        return features.cpu().numpy()\n",
    "    \n",
    "    def calculate_fid(self, real_features, fake_features):\n",
    "        \"\"\"Calculate FID between real and fake features\"\"\"\n",
    "        # Calculate mean and covariance\n",
    "        mu1 = real_features.mean(axis=0)\n",
    "        mu2 = fake_features.mean(axis=0)\n",
    "        sigma1 = np.cov(real_features, rowvar=False)\n",
    "        sigma2 = np.cov(fake_features, rowvar=False)\n",
    "        \n",
    "        # Numeric stabilization\n",
    "        eps = 1e-6\n",
    "        sigma1 = sigma1 + eps * np.eye(sigma1.shape[0], dtype=sigma1.dtype)\n",
    "        sigma2 = sigma2 + eps * np.eye(sigma2.shape[0], dtype=sigma2.dtype)\n",
    "        \n",
    "        # Calculate FID\n",
    "        diff = mu1 - mu2\n",
    "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        covmean = np.real_if_close(covmean)\n",
    "        \n",
    "        fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n",
    "        return float(np.real(fid))\n",
    "\n",
    "# FID evaluation with mixed precision support\n",
    "def evaluate_with_mixed_precision(generator, test_loader, device, num_samples=1000):\n",
    "    \"\"\"Run FID evaluation with mixed precision support\"\"\"\n",
    "    print(\"Starting FID evaluation with mixed precision...\")\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    # fid_calc = FIDCalculator(device)\n",
    "    \n",
    "    # Collect real and generated images\n",
    "    real_features = []\n",
    "    generated_features = []\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    print(\"Generating images for FID evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            if i >= num_samples // batch_size:\n",
    "                break\n",
    "                \n",
    "            images = batch['right_images'].to(device)\n",
    "            embeddings = batch['right_embed'].to(device)\n",
    "            \n",
    "            # MIXED PRECISION: Use autocast for generation\n",
    "            with autocast():\n",
    "                # Generate images\n",
    "                noise = torch.randn(images.size(0), 100, 1, 1, device=device)\n",
    "                generated_images = generator(noise, embeddings)\n",
    "            \n",
    "            # Extract features for FID (these need to be in full precision)\n",
    "            real_feat = fid_calc.get_features(images)\n",
    "            fake_feat = fid_calc.get_features(generated_images)\n",
    "            real_features.append(real_feat)\n",
    "            generated_features.append(fake_feat)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} batches...\")\n",
    "    \n",
    "    # Concatenate all features\n",
    "    real_features = np.concatenate(real_features, axis=0)\n",
    "    generated_features = np.concatenate(generated_features, axis=0)\n",
    "    \n",
    "    # Calculate FID\n",
    "    print(\"Calculating FID...\")\n",
    "    fid_score = fid_calc.calculate_fid(real_features, generated_features)\n",
    "    \n",
    "    print(f\"FID Score: {fid_score:.4f}\")\n",
    "    return fid_score\n",
    "\n",
    "# IS (Inception Score) Calculation\n",
    "class ISCalculator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        # Load pre-trained Inception v3 with robust loading\n",
    "        try:\n",
    "            from torchvision.models import Inception_V3_Weights\n",
    "            self.inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=False).to(device)\n",
    "        except Exception:\n",
    "            self.inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "        self.inception.eval()\n",
    "        \n",
    "        # Add resize layer for 32x32 images to work with Inception v3 (expects 299x299)\n",
    "        self.resize = transforms.Resize((299, 299))\n",
    "        \n",
    "    def calculate_is(self, images, splits=10):\n",
    "        \"\"\"Calculate Inception Score\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Resize 32x32 images to 299x299 for Inception v3\n",
    "            resized_images = self.resize(images)\n",
    "            # Get predictions\n",
    "            preds = F.softmax(self.inception(resized_images), dim=1)\n",
    "            \n",
    "            # Split into groups\n",
    "            split_size = len(preds) // splits\n",
    "            scores = []\n",
    "            \n",
    "            for i in range(splits):\n",
    "                start_idx = i * split_size\n",
    "                end_idx = start_idx + split_size\n",
    "                subset = preds[start_idx:end_idx]\n",
    "                \n",
    "                # Calculate IS for this subset\n",
    "                py = subset.mean(0)\n",
    "                scores.append(subset * (subset.log() - py.log()).sum(1).mean())\n",
    "                \n",
    "            return torch.stack(scores).mean().exp().item()\n",
    "\n",
    "# CLIP Score Calculation (Text-Image Matching)\n",
    "class CLIPCalculator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        # Note: This is a simplified CLIP implementation\n",
    "        # For full CLIP, you would need to install transformers and clip\n",
    "        self.text_encoder = None  # Placeholder for CLIP text encoder\n",
    "        self.image_encoder = None  # Placeholder for CLIP image encoder\n",
    "        \n",
    "    def calculate_clip_score(self, images, texts):\n",
    "        \"\"\"Calculate CLIP score between images and texts\"\"\"\n",
    "        # Simplified implementation - in practice, you'd use actual CLIP model\n",
    "        # This is a placeholder that returns a random score\n",
    "        return np.random.uniform(0.5, 0.9, len(images))\n",
    "\n",
    "# Text-Image Matching Evaluation\n",
    "def evaluate_text_image_matching(generator, test_loader, device, num_samples=100):\n",
    "    \"\"\"Evaluate how well generated images match their text descriptions\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    matching_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            if i >= num_samples // batch_size:\n",
    "                break\n",
    "                \n",
    "            images = batch['right_images'].to(device)\n",
    "            embeddings = batch['right_embed'].to(device)\n",
    "            \n",
    "            # Generate images\n",
    "            noise = torch.randn(images.size(0), 100, 1, 1, device=device)\n",
    "            generated_images = generator(noise, embeddings)\n",
    "            \n",
    "            # Calculate similarity (simplified)\n",
    "            similarity = F.cosine_similarity(\n",
    "                images.view(images.size(0), -1),\n",
    "                generated_images.view(generated_images.size(0), -1)\n",
    "            )\n",
    "            \n",
    "            matching_scores.extend(similarity.cpu().numpy())\n",
    "    \n",
    "    return np.mean(matching_scores), np.std(matching_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cab537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Evaluation Functions\n",
    "# #FID (Fréchet Inception Distance) Calculation\n",
    "# class FIDCalculator:\n",
    "#     def __init__(self, device):\n",
    "#         self.device = device\n",
    "#         # Load pre-trained Inception v3\n",
    "#         self.inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "#         self.inception.eval()\n",
    "        \n",
    "#         # Remove final classification layer\n",
    "#         self.inception.fc = nn.Identity()\n",
    "        \n",
    "#         # Add resize layer for 32x32 images to work with Inception v3 (expects 299x299)\n",
    "#         self.resize = transforms.Resize((299, 299))\n",
    "        \n",
    "#     def get_features(self, images):\n",
    "#         \"\"\"Extract features from images using Inception v3\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             # Resize 32x32 images to 299x299 for Inception v3\n",
    "#             resized_images = self.resize(images)\n",
    "#             features = self.inception(resized_images)\n",
    "#         return features.cpu().numpy()\n",
    "    \n",
    "#     def calculate_fid(self, real_features, fake_features):\n",
    "#         \"\"\"Calculate FID between real and fake features\"\"\"\n",
    "#         # Calculate mean and covariance\n",
    "#         mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "#         mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "        \n",
    "#         # Calculate FID\n",
    "#         diff = mu1 - mu2\n",
    "#         covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        \n",
    "#         if np.iscomplexobj(covmean):\n",
    "#             covmean = covmean.real\n",
    "            \n",
    "#         fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n",
    "#         return fid\n",
    "\n",
    "# # IS (Inception Score) Calculation\n",
    "# class ISCalculator:\n",
    "#     def __init__(self, device):\n",
    "#         self.device = device\n",
    "#         # Load pre-trained Inception v3\n",
    "#         self.inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "#         self.inception.eval()\n",
    "        \n",
    "#         # Add resize layer for 32x32 images to work with Inception v3 (expects 299x299)\n",
    "#         self.resize = transforms.Resize((299, 299))\n",
    "        \n",
    "#     def calculate_is(self, images, splits=10):\n",
    "#         \"\"\"Calculate Inception Score\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             # Resize 32x32 images to 299x299 for Inception v3\n",
    "#             resized_images = self.resize(images)\n",
    "#             # Get predictions\n",
    "#             preds = F.softmax(self.inception(resized_images), dim=1)\n",
    "            \n",
    "#             # Split into groups\n",
    "#             split_size = len(preds) // splits\n",
    "#             scores = []\n",
    "            \n",
    "#             for i in range(splits):\n",
    "#                 start_idx = i * split_size\n",
    "#                 end_idx = start_idx + split_size\n",
    "#                 subset = preds[start_idx:end_idx]\n",
    "                \n",
    "#                 # Calculate IS for this subset\n",
    "#                 py = subset.mean(0)\n",
    "#                 scores.append(subset * (subset.log() - py.log()).sum(1).mean())\n",
    "                \n",
    "#             return torch.stack(scores).mean().exp().item()\n",
    "\n",
    "# # CLIP Score Calculation (Text-Image Matching)\n",
    "# class CLIPCalculator:\n",
    "#     def __init__(self, device):\n",
    "#         self.device = device\n",
    "#         # Note: This is a simplified CLIP implementation\n",
    "#         # For full CLIP, you would need to install transformers and clip\n",
    "#         self.text_encoder = None  # Placeholder for CLIP text encoder\n",
    "#         self.image_encoder = None  # Placeholder for CLIP image encoder\n",
    "        \n",
    "#     def calculate_clip_score(self, images, texts):\n",
    "#         \"\"\"Calculate CLIP score between images and texts\"\"\"\n",
    "#         # Simplified implementation - in practice, you'd use actual CLIP model\n",
    "#         # This is a placeholder that returns a random score\n",
    "#         return np.random.uniform(0.5, 0.9, len(images))\n",
    "\n",
    "# # Text-Image Matching Evaluation\n",
    "# def evaluate_text_image_matching(generator, test_loader, device, num_samples=100):\n",
    "#     \"\"\"Evaluate how well generated images match their text descriptions\"\"\"\n",
    "#     generator.eval()\n",
    "    \n",
    "#     matching_scores = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(test_loader):\n",
    "#             if i >= num_samples // batch_size:\n",
    "#                 break\n",
    "                \n",
    "#             images = batch['right_images'].to(device)\n",
    "#             embeddings = batch['right_embed'].to(device)\n",
    "            \n",
    "#             # Generate images\n",
    "#             noise = torch.randn(images.size(0), 100, 1, 1, device=device)\n",
    "#             generated_images = generator(noise, embeddings)\n",
    "            \n",
    "#             # Calculate similarity (simplified)\n",
    "#             similarity = F.cosine_similarity(\n",
    "#                 images.view(images.size(0), -1),\n",
    "#                 generated_images.view(generated_images.size(0), -1)\n",
    "#             )\n",
    "            \n",
    "#             matching_scores.extend(similarity.cpu().numpy())\n",
    "    \n",
    "#     return np.mean(matching_scores), np.std(matching_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a480dec",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Visualization\n",
    "\n",
    "This section contains comprehensive evaluation metrics and visualization tools for the trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSSES \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generator loss plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Generator Loss During Training\")\n",
    "plt.plot(G_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Discriminator loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Discriminator Loss During Training\")\n",
    "plt.plot(D_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save plots\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator Loss During Training\")\n",
    "plt.plot(G_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_save_path, 'output_generatorLoss_{}.png'.format(date)))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Discriminator Loss During Training\")\n",
    "plt.plot(D_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_save_path, 'output_discriminatorLoss_{}.png'.format(date)))\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss plots saved successfully!\")\n",
    "print(f\"Generator losses: {len(G_losses)} points\")\n",
    "print(f\"Discriminator losses: {len(D_losses)} points\")\n",
    "print(\"Training completed successfully with proper loss tracking!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_evaluation(generator, test_loader, device, num_samples=1000):\n",
    "    \"\"\"Run comprehensive evaluation of the trained model\"\"\"\n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    is_calc = ISCalculator(device)\n",
    "    clip_calc = CLIPCalculator(device)\n",
    "    \n",
    "    # Collect real and generated images\n",
    "    real_images = []\n",
    "    generated_images = []\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    print(\"Generating images for evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            if i >= num_samples // batch_size:\n",
    "                break\n",
    "                \n",
    "            images = batch['right_images'].to(device)\n",
    "            embeddings = batch['right_embed'].to(device)\n",
    "            \n",
    "            # Store real images\n",
    "            real_images.append(images.cpu())\n",
    "            \n",
    "            # Generate fake images with mixed precision\n",
    "            with autocast():\n",
    "                noise = torch.randn(images.size(0), 100, 1, 1, device=device)\n",
    "                fake_images = generator(noise, embeddings)\n",
    "            generated_images.append(fake_images.cpu())\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} batches...\")\n",
    "    \n",
    "    # Calculate FID with mixed precision\n",
    "    print(\"Calculating FID with mixed precision...\")\n",
    "    fid_score = evaluate_with_mixed_precision(generator, test_loader, device, num_samples)\n",
    "    \n",
    "    # Calculate IS\n",
    "    print(\"Calculating IS...\")\n",
    "    generated_tensor = torch.cat(generated_images, dim=0).to(device)\n",
    "    is_score = is_calc.calculate_is(generated_tensor)\n",
    "    \n",
    "    # Calculate text-image matching\n",
    "    print(\"Calculating text-image matching...\")\n",
    "    matching_mean, matching_std = evaluate_text_image_matching(generator, test_loader, device, num_samples)\n",
    "    \n",
    "    # Calculate CLIP score (simplified)\n",
    "    print(\"Calculating CLIP score...\")\n",
    "    clip_scores = clip_calc.calculate_clip_score(generated_tensor, [\"test\"] * len(generated_tensor))\n",
    "    clip_mean = np.mean(clip_scores)\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'FID': fid_score,\n",
    "        'IS': is_score,\n",
    "        'Text_Image_Matching_Mean': matching_mean,\n",
    "        'Text_Image_Matching_Std': matching_std,\n",
    "        'CLIP_Score_Mean': clip_mean,\n",
    "        'num_samples': num_samples,\n",
    "        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"FID Score: {fid_score:.4f}\")\n",
    "    print(f\"IS Score: {is_score:.4f}\")\n",
    "    print(f\"Text-Image Matching: {matching_mean:.4f} ± {matching_std:.4f}\")\n",
    "    print(f\"CLIP Score: {clip_mean:.4f}\")\n",
    "    \n",
    "    return results, real_images, generated_images\n",
    "\n",
    "def save_evaluation_results(results, output_path):\n",
    "    \"\"\"Save evaluation results to file\"\"\"\n",
    "    results_path = os.path.join(output_path, f'evaluation_results_{results[\"timestamp\"]}.json')\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    json_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            json_results[key] = value.tolist()\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            json_results[key] = float(value)\n",
    "        else:\n",
    "            json_results[key] = value\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation results saved to: {results_path}\")\n",
    "    return results_path\n",
    "\n",
    "def create_evaluation_visualizations(real_images, generated_images, results, output_path):\n",
    "    \"\"\"Create visualizations for evaluation results\"\"\"\n",
    "    # Create comparison grid\n",
    "    real_grid = torch.cat(real_images[:16], dim=0)\n",
    "    fake_grid = torch.cat(generated_images[:16], dim=0)\n",
    "    \n",
    "    # Denormalize images for display\n",
    "    real_grid = (real_grid + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "    fake_grid = (fake_grid + 1) / 2\n",
    "    \n",
    "    # Create comparison figure\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Real images\n",
    "    real_img = vutils.make_grid(real_grid, nrow=4, normalize=True)\n",
    "    axes[0].imshow(real_img.permute(1, 2, 0))\n",
    "    axes[0].set_title('Real Images')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Generated images\n",
    "    fake_img = vutils.make_grid(fake_grid, nrow=4, normalize=True)\n",
    "    axes[1].imshow(fake_img.permute(1, 2, 0))\n",
    "    axes[1].set_title('Generated Images')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, f'evaluation_comparison_{results[\"timestamp\"]}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create metrics plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    metrics = ['FID', 'IS', 'Text-Image Matching', 'CLIP Score']\n",
    "    values = [results['FID'], results['IS'], results['Text_Image_Matching_Mean'], results['CLIP_Score_Mean']]\n",
    "    \n",
    "    bars = ax.bar(metrics, values, color=['red', 'blue', 'green', 'orange'])\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Evaluation Metrics')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, f'evaluation_metrics_{results[\"timestamp\"]}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c31b23",
   "metadata": {},
   "source": [
    "## Main Execution: Training with Checkpointing and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635556d",
   "metadata": {},
   "source": [
    "### Run Comprehensive Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "from torch.nn import functional as F\n",
    "from scipy import linalg\n",
    "# Run comprehensive evaluation\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"STARTING COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the final trained model\n",
    "print(\"Loading final trained model...\")\n",
    "generator.load_state_dict(torch.load(os.path.join(model_save_path, 'generator_final.pth')))\n",
    "generator.eval()\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results, real_images, generated_images = run_comprehensive_evaluation(\n",
    "    generator, test_loader, device, num_samples=1000  # Use 1000 samples\n",
    ")\n",
    "\n",
    "# Save evaluation results\n",
    "results_path = save_evaluation_results(evaluation_results, output_save_path)\n",
    "\n",
    "# Create visualizations\n",
    "create_evaluation_visualizations(real_images, generated_images, evaluation_results, output_save_path)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "print(f\"Visualizations saved to: {output_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2431496",
   "metadata": {},
   "source": [
    "### Resume Training (If Interrupted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify this cell if you need to resume training from a checkpoint\n",
    "# \n",
    "# # Example: Resume from epoch 50 checkpoint\n",
    "# checkpoint_path = 'saved_models/checkpoint_epoch_50.pth'\n",
    "# start_epoch, G_losses, D_losses = load_checkpoint(\n",
    "#     checkpoint_path, generator, discriminator, optimizer_G, optimizer_D\n",
    "# )\n",
    "# \n",
    "# # Continue training from start_epoch\n",
    "# print(f\"Resuming training from epoch {start_epoch}\")\n",
    "# \n",
    "# # Then run the training loop again from start_epoch to num_epochs\n",
    "# for epoch in range(start_epoch, num_epochs):\n",
    "#     # ... training code ...\n",
    "#     pass\n",
    "\n",
    "print(\"Resume training cell ready. Uncomment and modify as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e3ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Visualization\n",
    "\n",
    "# Load and display the training progress GIF\n",
    "# from IPython.display import Image\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "# Load the GIF\n",
    "# with open(os.path.join(output_save_path, 'output_gif_{}.gif'.format(date)),'rb') as file:\n",
    "#     display(Image(file.read()))\n",
    "with open(os.path.join(output_save_path, 'output_gif_{}.gif'.format(date)), 'rb') as f:\n",
    "    display(IPyImage(data=f.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
